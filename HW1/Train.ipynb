{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316108\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.498855\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.304533\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.351105\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.244809\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.227921\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.358895\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.243511\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.360837\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.127524\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9399/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.141075\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.118266\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.326621\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.187633\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.247620\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.080821\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.213559\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.162606\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.156757\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.116401\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9599/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.060435\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.167154\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.136564\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.205468\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.033662\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.155635\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.018674\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.252734\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.050664\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.146002\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9710/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.106740\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.104970\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.141630\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.113447\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.063432\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.047396\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.011565\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.011602\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.044782\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.149831\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9714/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.029634\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.009611\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.070628\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.145199\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.050439\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.008098\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.085153\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.056452\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.041376\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.127590\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9704/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.094479\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.058842\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.118688\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.067886\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.006101\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.054434\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.057073\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.020245\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.048105\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.006050\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9707/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.035439\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.006149\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.119139\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.117554\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.033564\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.020826\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.054022\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.017660\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.008723\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.093104\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9715/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.030255\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 設定裝置為 CUDA 如果可用，否則為 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 下載並準備 MNIST 資料集\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# 定義神經網絡模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # 平坦化\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 訓練模型\n",
    "def train(model, device, trainloader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(trainloader.dataset)} ({100. * batch_idx / len(trainloader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 測試模型\n",
    "def test(model, device, testloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # 累加批次損失\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # 找到概率最高的類別\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(testloader.dataset)} ({100. * correct / len(testloader.dataset):.0f}%)\\n')\n",
    "\n",
    "# 主程序\n",
    "for epoch in range(1, 11):  # 訓練 10 個 epoch\n",
    "    train(model, device, trainloader, optimizer, criterion, epoch)\n",
    "    test(model, device, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m     train_loop(\u001b[43mtrain_dataloader\u001b[49m, model, loss_fn, optimizer)\n\u001b[1;32m     54\u001b[0m     test_loop(test_dataloader, model, loss_fn)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
